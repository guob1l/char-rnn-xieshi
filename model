#encoding:'utf-8'
import collections
import numpy as np
import tensorflow as tf

#paremeter
batch_size=64
rnn_size=64
num_layer=2
epoch=20

path='poetry.txt'#each row for a  poetry

#读取文件each poetry as a element of list (contents)
def readfile(path):
    poetry = []
    with open(path,'r',encoding='utf-8') as f:
        lines=f.readlines()
        for line in lines:
            content=line.split(':')
            content=content[-1]
            if len(content)>79:
                continue
            content=content.strip()
            content='['+content+']'
            poetry.append(content)
    poetry = sorted(poetries, key=lambda x: len(x))
    return poetry

poetries=readfile(path)
print('唐诗总数: ', len(poetries))


#处理文件返回文本的数字化表示和字典
def process(poetries):
    words=[' ']
    for poetry in poetries:
        for word in poetry:
            words.append(word)

    count=collections.Counter(words)
    count_pairs=sorted(count.items(),key=lambda x:x[-1])
    all_word,_=zip(*count_pairs)
    vocab_size=len(all_word)
    vocab=dict(zip(all_word,range(vocab_size)))
    vocab_verse=dict(zip(vocab.values(),vocab.keys()))
    poetry_content = [list(map( vocab.get, poety)) for poety in poetries]

    return  poetry_content,vocab,vocab_verse,vocab_size


poetry_content,vocab,vocab_verse,vocab_size=process(poetries)
print('vocab_size:', vocab_size)



def create_bathes(poetry_content,batch_size,vocab):
    n_chunk=len(poetry_content)//batch_size
    x_batches=[]
    y_batches=[]

    for i in range(n_chunk):

        start_index=i*batch_size
        end_index=start_index+batch_size
        batches=poetry_content[start_index:end_index]

        max_length=max(map(len,batches))#一个batch中最长的sequence 长度。
        xdata=np.full((batch_size,max_length),vocab[' '],np.int32)#用空格填充矩阵。shape[batch_size,seq_length]
        for row in range(batch_size):
            xdata[row,:len(batches[row])]=batches[row]
        ydata=np.copy(xdata)
        ydata[:, :-1]=xdata[:, 1:]
        x_batches.append(xdata)
        y_batches.append(ydata)
    return x_batches,y_batches,n_chunk

xbatches,y_batches,n_chunk=create_bathes(poetry_content,batch_size,vocab)


def rnn_model(vocab_size,n_chunk,batch_size,rnn_size,num_layer,epoch):
    cells=[]
    cell=tf.contrib.rnn.BasicLSTMCell(rnn_size,state_is_tuple=True)
    for i in range(num_layer):
        cells.append(cell)
    cell=tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)
    initial_state=cell.zero_state(batch_size,tf.float32)

    # PlaceHolder:input_data,target_data
    input_data=tf.placeholder(dtype=tf.int32,shape=[batch_size,None])#shape [batch_size,seq_length]seq_leng不同批次的不同
    target_data=tf.placeholder(dtype=tf.int32,shape=[batch_size,None])
    #define w,b. convert number inputdata to embed_input
    with tf.variable_scope('rnn'):
        softmax_w=tf.get_variable('softmax_w',[rnn_size,vocab_size])
        softmax_b=tf.get_variable('softmax_b',[vocab_size])
        with tf.device('/cpu:0'):
            embedding=tf.get_variable('embedding',shape=[vocab_size,rnn_size])
            embed_input=tf.nn.embedding_lookup(embedding,input_data)


    outputs,last_state=tf.nn.dynamic_rnn(cell,embed_input,initial_state=initial_state)
    outputs=tf.reshape(outputs,[-1,rnn_size])
    logits=tf.matmul(outputs,softmax_w)+softmax_b
    probs=tf.nn.softmax(logits)
    targets=tf.reshape(target_data,[-1])

    loss=tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits],[targets],[tf.ones_like(targets,dtype=tf.float32)])
    cost=tf.reduce_mean(loss)
    lr=tf.Variable(0.0,trainable=False)
    tvars=tf.trainable_variables()
    grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),5)
    with tf.name_scope('optimer'):
        optimer=tf.train.AdamOptimizer(lr)
    train_op=optimer.apply_gradients(zip(grads,tvars))

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        saver=tf.train.Saver(tf.global_variables())
        for e in range(epoch):
            sess.run(tf.assign(lr,0.002*(0.97*e)))
            n=0
            for batch in range(n_chunk):
                train_loss,_,_=sess.run([cost,last_state,train_op],feed_dict={input_data:xbatches[n],target_data:y_batches[n]})
                n+=1
                print(e,batch,train_loss)

            saver.save(sess,'./save/model.ckpt',global_step=e)
            print('model saved')

rnn_model(vocab_size,n_chunk,batch_size,rnn_size,num_layer,epoch)
